Large Movie Review Dataset v1.0

Overview

This dataset contains movie reviews along with their associated binary
sentiment polarity labels. It is intended to serve as a benchmark for
sentiment classification. This document outlines how the dataset was
gathered, and how to use the files provided. 

Dataset 

The core dataset contains 50,000 reviews split evenly into 25k train
and 25k test sets. The overall distribution of labels is balanced (25k
pos and 25k neg). We also include an additional 50,000 unlabeled
documents for unsupervised learning. 

In the entire collection, no more than 30 reviews are allowed for any
given movie because reviews for the same movie tend to have correlated
ratings. Further, the train and test sets contain a disjoint set of
movies, so no significant performance is obtained by memorizing
movie-unique terms and their associated with observed labels.  In the
labeled train/test sets, a negative review has a score <= 4 out of 10,
and a positive review has a score >= 7 out of 10. Thus reviews with
more neutral ratings are not included in the train/test sets. In the
unsupervised set, reviews of any rating are included and there are an
even number of reviews > 5 and <= 5.

Files

There are two top-level directories [train/, test/] corresponding to
the training and test sets. Each contains [pos/, neg/] directories for
the reviews with binary labels positive and negative. Within these
directories, reviews are stored in text files named following the
convention [[id]_[rating].txt] where [id] is a unique id and [rating] is
the star rating for that review on a 1-10 scale. For example, the file
[test/pos/200_8.txt] is the text for a positive-labeled test set
example with unique id 200 and star rating 8/10 from IMDb. The
[train/unsup/] directory has 0 for all ratings because the ratings are
omitted for this portion of the dataset.

We also include the IMDb URLs for each review in a separate
[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will
have its URL on line 200 of this file. Due the ever-changing IMDb, we
are unable to link directly to the review, but only to the movie's
review page.

In addition to the review text files, we include already-tokenized bag
of words (BoW) features that were used in our experiments. These 
are stored in .feat files in the train/test directories. Each .feat
file is in LIBSVM format, an ascii sparse-vector format for labeled
data.  The feature indices in these files start from 0, and the text
tokens corresponding to a feature index is found in [imdb.vocab]. So a
line with 0:7 in a .feat file means the first word in [imdb.vocab]
(the) appears 7 times in that review.

LIBSVM page for details on .feat file format:
http://www.csie.ntu.edu.tw/~cjlin/libsvm/

We also include [imdbEr.txt] which contains the expected rating for
each token in [imdb.vocab] as computed by (Potts, 2011). The expected
rating is a good way to get a sense for the average polarity of a word
in the dataset.

Citing the dataset

When using this dataset please cite our ACL 2011 paper which
introduces it. This paper also contains classification results which
you may want to compare against.

************************************************************************************

Preprocessing:

First, we split the data into a train (75%) set and a validation (25%) set. Each set includes negative and positive reviews that are combined by a random mix.
Negative reviews get a '0' label, and positive reviews get a '1' label. The train set contains 28,126 files and the validation set contains 9,374 mixed files. 
For the GRU network, we draw weights from a normal distribution (m=0, std=0.01).

Embedding methods:

The embedding vector of each word that enters the network is a length of 100, and the output is 50. 
We ran the two methods on 10 epochs. For each method, we created a BCELoss network.
    
    1. Word to Vec - We continued to train this model with the list of words from all the reviews in the data set, 
                     so we avoided a situation where we faced a world that has no vector embedding representation. 
    2. Fast text - We re-created this embedding configuration based on the same list of words from all our reviews on the data set. 

Results and Conclusions:

To compare the 2 embedding methods and to examine the quality of the model, we used loss function and accuracy index.

In terms of the loss func, it can be seen from the graph that the minimum loss value is obtained from embedding method 1 (Word2Vec). 
As expected, the accuracy of the train set in the embedding method 1 (59.0912) is higher than the embedding method 2 (58.6219) based on the last epoch.
Respectively, this can be seen for val set. The gaps are very small so they cannot be seen in the graph.
In addition, in the case of overfitting, it can be seen from the 2 graphs that the accuracy of both train set and val set is increasing
and therefore it can not be said that there is overfitting. 
However, due to long run times, we had to run the model for only 10 epochs, so it can be assumed that we did not drain the learning process of the model,
and we can continue to train it to get better results.
We also noticed in the graphs that there is a rapid convergence of the data (already starting from the second restraint). 
First, we can assume that over more epochs there will be more changes. Second, we can assume that the cause is a coincidence in the run. 
To make sure this is not a coincidence in the run, we ran the training on smaller samples as well. 
The first sample contained 100 samples on train set and 100 samples on val set. 
The second sample contained 4500 samples on train set and 1500 samples on val set (to maintain the ratio of 75% and 25%). 
From this test, it can be seen that as the number of samples increases the convergence is faster, 
and therefore we conclude that the graphs on the whole data are not random. The graphs of the tests are in the code file.

************************************************************************************

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

References

Potts, Christopher. 2011. On the negativity of negation. In Nan Li and
David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,
636-659.

Contact

For questions/comments/corrections please contact Andrew Maas
amaas@cs.stanford.edu
